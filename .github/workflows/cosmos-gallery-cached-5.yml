name: Cosmos Gallery cached 5 -> gallery.json

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"   # hourly

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    env:
      URLS: >
        https://www.cosmos.so/rlphoto/swim,
        https://www.cosmos.so/rlphoto/studio-tests,
        https://www.cosmos.so/rlphoto/studio-test-feminine,
        https://www.cosmos.so/rlphoto/swim-resort,
        https://www.cosmos.so/rlphoto/location-tests
      MAX_SCROLLS: 200
      WAIT_BETWEEN: 1000
      FIRST_IDLE: 9000
      STABLE_CHECKS: 6
      PLAYWRIGHT_BROWSERS_PATH: "~/.cache/ms-playwright"

    steps:
      - name: Checkout repository (force latest main)
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          clean: true

      - name: Use Node.js 20 + cache npm
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Prepare cache dirs
        run: |
          mkdir -p ~/.cache/ms-playwright
          mkdir -p ~/.cache/playwright
          mkdir -p node_modules

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: |
            /home/runner/.cache/ms-playwright
            /home/runner/.cache/playwright
          key: ms-playwright-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ms-playwright-${{ runner.os }}-

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: node_modules
          key: node-modules-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            node-modules-${{ runner.os }}-

      - name: Install dependencies (reproducible)
        run: npm ci

      - name: Install Playwright (Chromium + deps)
        run: npx playwright install --with-deps chromium

      - name: Scrape all Cosmos pages
        run: |
          IFS=',' read -ra arr <<< "${URLS}"
          mkdir -p public
          for u in "${arr[@]}"; do
            u=$(echo "$u" | xargs)
            slug=$(basename "$u")
            echo "Scraping $u -> public/${slug}.json"
            COSMOS_URL="$u" OUT_FILE="public/${slug}.json" \
            MAX_SCROLLS="${MAX_SCROLLS}" WAIT_BETWEEN="${WAIT_BETWEEN}" FIRST_IDLE="${FIRST_IDLE}" STABLE_CHECKS="${STABLE_CHECKS}" \
            npm run scrape
          done

      # ðŸ”§ Only upgrade Mux lowâ†’high and dedupe exact URLs. Do NOT drop Cosmos MP4s.
      - name: Normalize + dedupe (Mux lowâ†’high, no host preference)
        run: |
          node - <<'NODE'
          const fs = require('fs');
          const path = require('path');

          const PUB = path.resolve(process.cwd(), 'public');
          if (!fs.existsSync(PUB)) process.exit(0);

          const files = fs.readdirSync(PUB).filter(f => f.endsWith('.json'));

          const SKIP_AVATAR = /cdn\.cosmos\.so\/default-avatars\//i;
          const MUX_HOST = 'stream.mux.com';

          const normUrl = (url) => {
            try {
              const u = new URL(url);
              return `${u.protocol}//${u.host.toLowerCase()}${u.pathname}`;
            } catch { return null; }
          };

          const upgradeMux = (url) => {
            try {
              const u = new URL(url);
              if (u.host.toLowerCase() !== MUX_HOST) return url;
              const parts = u.pathname.split('/').filter(Boolean);
              if (parts.length >= 2) {
                parts[1] = 'high.mp4'; // force high.mp4
                u.pathname = '/' + parts.join('/');
                u.search = '';
                u.hash = '';
              }
              return u.toString();
            } catch { return url; }
          };

          const detectType = (url, fallback='image') =>
            /\.(mp4|webm|m4v|mov)(\?|$)/i.test(url) ? 'video' : fallback;

          for (const file of files) {
            const full = path.join(PUB, file);
            let data;
            try { data = JSON.parse(fs.readFileSync(full, 'utf8')); }
            catch { continue; }

            const list = Array.isArray(data) ? data
                        : Array.isArray(data.items) ? data.items
                        : [];

            const seen = new Set();
            const out  = [];

            for (const raw of list) {
              const item = { ...raw };

              // choose src field
              let src = item.src || item.url || item.image || item.href;
              if (!src) continue;
              if (SKIP_AVATAR.test(src)) continue;

              // upgrade mux lowâ†’high first, then normalize
              src = upgradeMux(src);
              const nsrc = normUrl(src);
              if (!nsrc) continue;

              // normalize poster if present (no special handling)
              if (item.poster) {
                const p = normUrl(item.poster);
                if (p) item.poster = p;
              }

              // type detection
              const type = (item.type || detectType(nsrc, 'image')).toLowerCase();

              // dedupe by normalized URL + type (keeps first seen)
              const key = `${type}:${nsrc}`;
              if (seen.has(key)) continue;
              seen.add(key);

              item.type = type;
              item.src  = nsrc;

              out.push(item);
            }

            // write back in the same structure we found
            let finalObj = data;
            if (Array.isArray(data)) finalObj = out;
            else if (Array.isArray(data.items)) finalObj = { ...data, items: out, count: out.length };
            else finalObj = out;

            fs.writeFileSync(full, JSON.stringify(finalObj, null, 2));
            console.log(`âœ” ${file}: ${list.length} â†’ ${out.length} (Mux high, simple dedupe)`);
          }
          NODE

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          publish_branch: gh-pages
